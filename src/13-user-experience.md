# User Experience

## Introduction

What is user experience?
[Dickson-Deane and Chen (2018)][dickson2018]
write that
"user experience determines
the quality of an interaction
being used by an actor in
order to achieve a specific outcome"
(Intro section, para 1).
[Parush (2017)][parush2017]
highlights adjacent terms
like *human-computer interaction (HCI)*
and *usability*.
Let's say then that HCI
encompasses the entire domain of
interaction between people and
computers and
how that interaction is designed and
that *user experience (UX)*
focuses on the quality of that interaction.
These are not precise definitions.
Some might use the terms *UX* and *HCI*
interchangeably.
As ERM librarians, though,
the job is to focus on the
quality of the patron's experience
with electronic services, and
this entails understanding both the
systems and technologies involved
and the users interacting
with these systems and technologies.

Dickson-Deane and Chen (2018) 
outline the parameters involved with UX.
Let me modify their example and
frame it within the context of an ERM
experience for a patron:

- **Actor**: A user of the web resource, like a library
  website.
- **Object**: The web resource, or some part of it.
- **Context**: The setting.
  - What's happening?
  - What's the motivation for use?
  - What's the background knowledge?
  - What's the action?
- **User Interface**: The tools available by the object and
  the look and feel.
  - More specifically, Parush (2017) states that "the user
    interface mediates between the user and computer" and it
    includes three basic components:
    - *controls*: The tools used to control and interact
      with the system: buttons, menus, voice commands,
      keyboards, etc.
    - *displays*: The information presented to the user or
      the hardware used to present the information: screens,
      speakers, etc.
    - *interactions and dialogues*: The exchange between the
      system and the user including the use of the controls
      and responding to feedback.
- **Interaction**: What the actor is doing with the UI
- **(Intended/Expected/Prior) User experience**: The
  intended or expected use of the object. The user's
  expectations based on prior use.
- **(Actual) User experience**: The actions that took place;
  the actions that had to be modified based on unintended
  results.

These parameters would be helpful
for devising a UX study that involves
observing patrons interacting with
a system and then interviewing them to
complete the details.
Note that the same kind of
systematic thinking can be
applied to evaluate other user experiences,
like those between a
librarian and
an electronic resource management system.
Often the focus is on patron user experience,
but it's just as important to
evaluate UX for librarians and
to consider UX when selecting
an ERM system or an ILS system.

In any case,
these parameters help us
step through and highlight the
complicated process of interacting
with a computer, generally, or
a library resource, more specifically.
As with many other topics we've
discussed here,
we can also incorporate these
parameters into a workflow for
evaluating UX.

## Complex Library Websites

It is due to the complexities
involved and a focus on the systems that
[Pennington (2015)][pennington2015]
argues for a more UX centered approach to
library website design.
Think about your own state of
knowledge of ERM before you started
learning about this area of librarianship.
For example,
now that you know somewhat
how link resolvers function,
your experience
using them as patrons and your understanding
of their technical aspects as librarians
provide you with a set of skills
and experiences that make you
more likely to identify the cause
of a malfunction if you find one.
With this ability to suss out an issue,
it becomes easier to solve,
and the experience itself involves less anxiety.
However, most users and patrons of these
systems will not have any technical knowledge
of these systems.
Thus, when these systems break on them,
their frustration with the **experience**
might lead to unfortunate outcomes:
they may not retrieve the information they need;
they may not reach out to a librarian for help; or
they may stop using the library's resources
in preference for something of inferior quality.
We need to remember that this happens,
and if possible,
to build in *proactive troubleshooting* processes
that anticipate and solve them before they do
happen to patrons.

Here's the crux, though.
As you gain more skill and
expertise with these systems,
you will eventually lose the ability
to see these systems as a novice user, and
that distance will only grow over time.
It is therefore,
as Pennington (2015) argues,
important to gather data from users.
User experience research nurtures
a user centered mindset.

Indeed, [Kraft et al. (2022)][kraft2022]
used focus groups and surveys to collect
user experience data on a library's
implementation of its A-Z Database List.
The results of this study are interesting.
As Kraft et al. (2022) point out,
librarians have long made efforts
to reduce the use of library terminology 
in their messaging to patrons,
since this only serves as a point of confusion.
However, their focus group participants
described contrasting opinions about how
color was used on the site, and
how color was used had some fairly
dramatic effects on which sources were selected
to pursue.

## The Data That Exists

In addition to user studies
that require conducting direct research
and reading prior studies that require
literature searches,
we should also know that
libraries already possess
a wealth of data to explore,
and this data could
provide needed insight.
Here, as we've learned before,
workflows play an important role
in applying mechanisms to track,
report, and fix problems with various
electronic resource systems.
[Browning (2015)][browning2015],
for example,
describes the use of [Bugzilla][bugzilla],
software that's commonly used for software
development for bug tracking and
generating reports about what breaks.
Once problems are identified,
they can be categorized and assigned
to facilitate quick solutions.
Thus, whereas one approach is to understand what
we can learn from data about usage
([Fry, 2016][pennington2016]),
Browning (2015)
describes what we can learn from
data about breakage.
Both kinds of data offer substantial
understanding about user experience.

## Conclusion

I agree with [McDonald (2016)][pennington2016]
that despite having around 30
or so years of experience
with web-based and other electronic resource types,
we are still in the throes of disruption.
There's much yet to learn about
design for the web,
just like there's a lot of
left to learn about how to
design a home or office,
and nothing will be settled for a while.
Although I doubt if there will
be any single dominate user experience or
user interface,
since there are many
cultures, backgrounds, and aesthetics,
I'm fairly sure the low-hanging
fruit problems will work out soon enough.
Remember though that 95% of the
cause of all of this complexity is due to
copyright issues,
which necessitate the entire
electronic resource ecosystem and
the complications that
introduced by working with vendors
who work with different,
but overlapping, publishers, etc.
If something were to change about copyright,
then it's a whole new ballgame.

On a final note,
you might be wondering how
*information seeking* is related
to *HCI* and to *UX*.
For example,
we learned from Kraft et al. (2022)
that color can influence what resources
patrons investigate.
Anytime we interact with a
computer (broadly speaking) in
order to seek information,
then we have an overlap with *UX*.
There are areas of non-overlap, too.
We don't always use computers to
look for information, and
we don't always look for
information on computers.
*UX* is like this, too.
*UX* is not always about computers
but can be about user experience generally.
I bring this up because if you do
become involved with *UX* work at a library
(or elsewhere),
then I'd encourage you to refer
also to the information seeking and
related literature when
it's appropriate to do so.
Remember, it's all about users and
it's also all interconnected.

## Readings / References

Browning, S. (2015). Data, Data, Everywhere, nor Any Time to
Think: DIY Analysis of E-Resource Access Problems. Journal
of Electronic Resources Librarianship, 27(1), 26–34.
[https://doi.org/10.1080/1941126X.2015.999521][browning2015]

Kraft, A., Scronce, G., & Jones, A. (2022). Virtual focus
groups for improved A-Z list user experience. The Journal of
Academic Librarianship, 48(4), 102541.
[https://doi.org/10.1016/j.acalib.2022.102541][kraft2022]

Pennington, B. (2015). ERM UX: Electronic Resources
Management and the User Experience. Serials Review, 41(3),
194–198.
[https://doi.org/10.1080/00987913.2015.1069527][pennington2015]

## Additional References

Adams, A. L., & Hanson, M. (2020). Primo on the Go: A
Usability Study of the Primo Mobile Interface. Journal of
Web Librarianship.
[http://www.tandfonline.com/doi/abs/10.1080/19322909.2020.1784820][adams2020]

Dickson-Deane, C., & Chen, H.-L. (Oliver). (2018).
Understanding user experience. In M. Khosrow-Pour (Ed.),
*Encyclopedia of Information Science and Technology* (Fourth
Edition, pp. 7599–7608). IGI Global.
[http://www.igi.global.com/chapter/understanding-user-experience/184455][dickson2018]

Hamlett, A., & Georgas, H. (2019). In the Wake of Discovery:
Student Perceptions, Integration, and Instructional Design.
Journal of Web Librarianship, 13(3), 230–245.
[https://doi.org/10.1080/19322909.2019.1598919][hamlett2019]

Parush, A. (2017). Human-computer interaction. In S. G.
Rogelberg (Ed.), The SAGE Encyclopedia of Industrial and
Organizational Psychology (2nd edition, pp. 669–674). SAGE
Publications, Inc.
[https://doi.org/10.4135/9781483386874.n229][parush2017]

Pennington, B., Chapman, S., Fry, A., Deschenes, A., &
McDonald, C. G. (2016). Strategies to Improve the User
Experience. Serials Review, 42(1), 47–58.
[https://doi.org/10.1080/00987913.2016.1140614][pennington2016]

[adams2020]:http://www.tandfonline.com/doi/abs/10.1080/19322909.2020.1784820
[browning2015]:https://doi.org/10.1080/1941126X.2015.999521
[dickson2018]:http://www.igi.global.com/chapter/understanding-user-experience/184455
[hamlett2019]:https://doi.org/10.1080/19322909.2019.1598919
[kraft2022]:https://doi.org/10.1016/j.acalib.2022.102541
[parush2017]:https://doi.org/10.4135/9781483386874.n229
[pennington2015]:https://doi.org/10.1080/00987913.2015.1069527
[pennington2016]:https://doi.org/10.1080/00987913.2016.1140614
[bugzilla]:https://www.bugzilla.org/

<!--
## Appendix

The suggested reading by Pennington, Chapman, Fry,
Deschenes, and McDonald (2016) is a contribution from four
librarians with theoretical and practical suggestions for UX
research. Both Pennington and Chapman note the best way to
measure UX or detect UX issues is to conduct research with
users (recall our conversations on proactive
troubleshooting), but this isn't always possible due to
financial, time, or other constraints. However, there is a
wealth of research on UX and in the absence of the ability
to conduct research, locating prior research and applying
that research to your setting is paramount. Drawing from the
literature, Chapman describes several important UX
principles that include:

* chunking
* highlighting and prominence
* KISS
* choice simplification
* choice reduction

If you can conduct a user study, then Deschenes offers helpful tips on
recruiting users. Remember that the users that you recruit should be actual
users of the systems. If you are interested in why some segments of the
population do not use the library's resources, then that would be a different
kind of study.
-->
